[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "work/research/index.html",
    "href": "work/research/index.html",
    "title": "Research",
    "section": "",
    "text": "Context State Decoupling: A Neuro-Symbolic Approach to Infinite Context\n\n\n\nResearch\n\nArchitecture\n\nNeuro-Symbolic AI\n\nSLM\n\n\n\n\n\n\n\n\n\nJan 4, 2026\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome to my AI Lab",
    "section": "",
    "text": "This is a proof-of-concept post demonstrating scientific publishing capabilities with Quarto."
  },
  {
    "objectID": "posts/welcome/index.html#mathematical-formulation",
    "href": "posts/welcome/index.html#mathematical-formulation",
    "title": "Welcome to my AI Lab",
    "section": "Mathematical Formulation",
    "text": "Mathematical Formulation\nWe can render complex equations using LaTeX. For example, the Cross-Entropy Loss function:\n\nL_{CE} = - \\sum_{i=1}^{C} t_i \\log(p_i)\n\nWhere: - t_i is the truth label. - p_i is the Softmax probability for the i^{th} class."
  },
  {
    "objectID": "posts/welcome/index.html#architecture-diagram",
    "href": "posts/welcome/index.html#architecture-diagram",
    "title": "Welcome to my AI Lab",
    "section": "Architecture Diagram",
    "text": "Architecture Diagram\nHere is a simple Neural Network visualizations using Mermaid.js:\n\n\n\n\n\ngraph LR\n    A[Input Layer] --&gt; B(Hidden Layer)\n    B --&gt; C{Activation}\n    C --&gt; D[Output Layer]\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style D fill:#ccf,stroke:#333,stroke-width:2px"
  },
  {
    "objectID": "posts/welcome/index.html#implementation",
    "href": "posts/welcome/index.html#implementation",
    "title": "Welcome to my AI Lab",
    "section": "Implementation",
    "text": "Implementation\nBelow is a PyTorch tensor initialization:\nimport torch\n\n# Define a simple tensor\nx = torch.tensor([[1, 2], [3, 4]])\nprint(f\"Tensor shape: {x.shape}\")"
  },
  {
    "objectID": "work/projects/index.html",
    "href": "work/projects/index.html",
    "title": "AI Experiments",
    "section": "",
    "text": "Here is a list of my repositories:\n\nCDLL\nCNN-for-Text-Classification\nK-Nearest-Neighbor"
  },
  {
    "objectID": "work/research/reasoning/context-state-decoupling/index.html",
    "href": "work/research/reasoning/context-state-decoupling/index.html",
    "title": "Context State Decoupling: A Neuro-Symbolic Approach to Infinite Context",
    "section": "",
    "text": "The scaling laws of Transformer architectures are currently bound by the quadratic cost of the attention mechanism, limiting the effective reasoning horizon of Large Language Models (LLMs). This paper proposes Context State Decoupling (CSD), a neuro-symbolic architecture that dissociates the logical state of a conversation (persisted as a Directed Acyclic Graph) from the ephemeral inference compute (the LLM). By treating the LLM not as a container of state but as a purely functional transition kernel, we demonstrate a pathway to effectively “infinite” context windows with constant memory complexity O(k) for Small Language Models (SLMs)."
  },
  {
    "objectID": "work/research/reasoning/context-state-decoupling/index.html#abstract",
    "href": "work/research/reasoning/context-state-decoupling/index.html#abstract",
    "title": "Context State Decoupling: A Neuro-Symbolic Approach to Infinite Context",
    "section": "",
    "text": "The scaling laws of Transformer architectures are currently bound by the quadratic cost of the attention mechanism, limiting the effective reasoning horizon of Large Language Models (LLMs). This paper proposes Context State Decoupling (CSD), a neuro-symbolic architecture that dissociates the logical state of a conversation (persisted as a Directed Acyclic Graph) from the ephemeral inference compute (the LLM). By treating the LLM not as a container of state but as a purely functional transition kernel, we demonstrate a pathway to effectively “infinite” context windows with constant memory complexity O(k) for Small Language Models (SLMs)."
  },
  {
    "objectID": "work/research/reasoning/context-state-decoupling/index.html#the-linear-paradox",
    "href": "work/research/reasoning/context-state-decoupling/index.html#the-linear-paradox",
    "title": "Context State Decoupling: A Neuro-Symbolic Approach to Infinite Context",
    "section": "1. The Linear Paradox",
    "text": "1. The Linear Paradox\nIn the standard Transformer paradigm, “intelligence” is strictly bound by the context window W. To reason about a sequence of length N, the model must maintain a Key-Value (KV) cache for all previous tokens.\nThe computational cost of the attention mechanism for a sequence of length N is traditionally:\n\n\\text{Cost}_{attn} \\propto O(N^2)\n\nWhile optimizations like FlashAttention and linear attention variants reduce this burden, they do not solve the semantic noise problem. As N \\to \\infty, the probability of attention heads attending to irrelevant historical tokens increases, degrading reasoning performance—a phenomenon known as “Lost in the Middle” [1].\nCurrently, systems address this via a Sliding Window approach, truncating history H_{t-k}. This results in catastrophic forgetting, where the “self” of the agent is destroyed every time the window shifts."
  },
  {
    "objectID": "work/research/reasoning/context-state-decoupling/index.html#the-decoupling-thesis",
    "href": "work/research/reasoning/context-state-decoupling/index.html#the-decoupling-thesis",
    "title": "Context State Decoupling: A Neuro-Symbolic Approach to Infinite Context",
    "section": "2. The Decoupling Thesis",
    "text": "2. The Decoupling Thesis\nWe propose that the conflation of State (Memory) and Compute (Inference) is a category error in current architectures. Drawing inspiration from the distinction between the Hippocampus (episodic memory) and the Prefrontal Cortex (working memory/reasoning), we propose Context State Decoupling (CSD).\nThe architecture separates the system into two distinct components:\n\nThe Logical State (Symbolic Substrate): An evolving, persistent Directed Acyclic Graph (DAG) storing facts, events, and causal dependencies.\nThe Inference Engine (Neural Substrate): An SLM acting as a stateless function f(x) that synthesizes state updates."
  },
  {
    "objectID": "work/research/reasoning/context-state-decoupling/index.html#graph-topology-as-memory",
    "href": "work/research/reasoning/context-state-decoupling/index.html#graph-topology-as-memory",
    "title": "Context State Decoupling: A Neuro-Symbolic Approach to Infinite Context",
    "section": "3. Graph Topology as Memory",
    "text": "3. Graph Topology as Memory\nInstead of a linear array of tokens T = [t_1, t_2, ..., t_N], the conversation state is stored as a graph G = (V, E).\n\nVertices (V): Atomic “Context Blocks”—semantically segmented units of user intent or system insight.\nEdges (E): Causal or temporal relationships between blocks (e.g., [User Request] -&gt; [Clarification]).\n\n\n\n\nFigure 1: The CSD Architecture vs Standard Linear Context. Visualizing the shift from linear buffering to cyclic graph retrieval."
  },
  {
    "objectID": "work/research/reasoning/context-state-decoupling/index.html#neuro-symbolic-retrieval",
    "href": "work/research/reasoning/context-state-decoupling/index.html#neuro-symbolic-retrieval",
    "title": "Context State Decoupling: A Neuro-Symbolic Approach to Infinite Context",
    "section": "4. Neuro-Symbolic Retrieval",
    "text": "4. Neuro-Symbolic Retrieval\nWhen the system receives input I_t, it constructs a Localized Context (C_{local}) rather than loading the full history. This is a retrieval operation defined as:\n\nC_{local} = R(I_t, G) = \\text{Top}_k(Sim(I_t, V)) \\cup N(\\text{Top}_k)\n\nWhere: * Sim is a cosine similarity function in vector space. * N represents the topological neighbors (parent/child nodes) in the DAG, preserving causal continuity."
  },
  {
    "objectID": "work/research/reasoning/context-state-decoupling/index.html#the-cognitive-loop",
    "href": "work/research/reasoning/context-state-decoupling/index.html#the-cognitive-loop",
    "title": "Context State Decoupling: A Neuro-Symbolic Approach to Infinite Context",
    "section": "4.1 The Cognitive Loop",
    "text": "4.1 The Cognitive Loop\nThe inference process becomes a recurring loop:\n\nSegmentation: Input is parsed into an atomic intent v_{new}.\nRetrieval: C_{local} is fetched from G.\nSynthesis: The SLM generates the response r and a state update u. \n(r, u) = \\text{SLM}(v_{new}, C_{local})\n\nUpdate: u is committed to G, creating edges based on the retrieval path."
  },
  {
    "objectID": "work/research/reasoning/context-state-decoupling/index.html#discussion",
    "href": "work/research/reasoning/context-state-decoupling/index.html#discussion",
    "title": "Context State Decoupling: A Neuro-Symbolic Approach to Infinite Context",
    "section": "5. Discussion",
    "text": "5. Discussion\nBy offloading “remembering” to a structured database, we reduce the cognitive load on the model. This implies that massive parameter counts (70B+) are not required for long-context coherence. A finely-tuned SLM (e.g., 3B-7B parameters), when equipped with CSD, can outperform larger models in tasks requiring high-fidelity recall over long horizons, as demonstrated in similar approaches like MemGPT [2] and GraphRAG [3]."
  },
  {
    "objectID": "work/research/reasoning/context-state-decoupling/index.html#references",
    "href": "work/research/reasoning/context-state-decoupling/index.html#references",
    "title": "Context State Decoupling: A Neuro-Symbolic Approach to Infinite Context",
    "section": "References",
    "text": "References\n\nLiu, N. F., et al. (2024). Lost in the Middle: How Language Models Use Long Contexts. arXiv preprint.\nPacker, C., et al. (2023). MemGPT: Towards LLMs as Operating Systems. arXiv:2310.08560.\nEdge, D., et al. (2024). From Local to Global: A Graph RAG Approach to Query-Focused Summarization. Microsoft Research."
  }
]